1)Mnist dataset:
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
# Check the shape of the data
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
# Display the number of classes
num_classes = len(np.unique(y_train))
print("Number of classes:", num_classes)
# Visualize one of the training images
plt.imshow(X_train[2])
plt.show()
# Normalize the data
X_train = X_train / 255.0
X_test = X_test / 255.0
# Build the neural network model
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(128, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(10, activation='softmax'))
# Display the model summary
model.summary()
# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
# Train the model
history = model.fit(X_train, y_train, epochs=30, validation_split=0.2)

# Make predictions on the test set
y_prob = model.predict(X_test)
y_pred = y_prob.argmax(axis=1)
# Assuming you have a list or array of class labels
class_labels = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7',
'class_8', 'class_9']
# Mapping indices to class labels
y_pred_classes = [class_labels[i] for i in y_pred]
# Calculate and display the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Plot training & validation loss values
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()
# Plot training & validation accuracy values
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()
# Visualize one of the test images
plt.imshow(X_test[5])
plt.show()
# Predict the class of a single test image
single_prediction = model.predict(X_test[1].reshape(1, 28, 28)).argmax(axis=1)
print("Predicted class for X_test[1]:", single_prediction[0])
1)cifar10:
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Conv2D, MaxPooling2D
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Load the CIFAR-10 dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()
# Check the shape of the data
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
# Display the number of classes
num_classes = len(np.unique(y_train))
print("Number of classes:", num_classes)
# Visualize one of the training images
plt.imshow(X_train[2])
plt.show()
# Normalize the data
X_train = X_train / 255.0
X_test = X_test / 255.0
# Build the neural network model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))
# Display the model summary
model.summary()
# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
# Train the model
history = model.fit(X_train, y_train, epochs=30, validation_split=0.2)
# Make predictions on the test set
y_prob = model.predict(X_test)
y_pred = y_prob.argmax(axis=1)

# Calculate and display the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Plot training & validation loss values
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()
# Plot training & validation accuracy values
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()
# Visualize one of the test images
plt.imshow(X_test[5])
plt.show()
# Predict the class of a single test image
single_prediction = model.predict(X_test[1].reshape(1, 32, 32, 3)).argmax(axis=1)
print("Predicted class for X_test[1]:", single_prediction[0])
2)Autoencoder:
from sklearn.datasets import make_classification
import pandas as pd
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import keras
from keras.layers import Dense
from keras.models import Sequential
from sklearn.metrics import classification_report
# Generate synthetic dataset
X, y = make_classification(n_samples=100000, n_features=32, n_informative=32,
n_redundant=0,
n_clusters_per_class=1, weights=[0.995, 0.005], class_sep=0.5,
random_state=0)

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Print dataset information
print('The number of records in the training dataset is', X_train.shape[0])
print('The number of records in the test dataset is', X_test.shape[0])
class_counts = sorted(Counter(y_train).items())
print(f"The training dataset has {class_counts[0][1]} records for the min occurring class.")
# Define and train the autoencoder
X_train_normal = X_train[np.where(y_train == 0)]
input = keras.layers.Input(shape=(32,))
encoder = Sequential([
Dense(16, activation='relu'),
Dense(8, activation='relu'),
Dense(4, activation='relu')
])(input)
decoder = Sequential([
Dense(8, activation="relu"),
Dense(16, activation="relu"),
Dense(32, activation="sigmoid")
])(encoder)
autoencoder = keras.Model(inputs=input, outputs=decoder)
autoencoder.summary()
autoencoder.compile(optimizer='adam', loss='mae')
history = autoencoder.fit(X_train_normal, X_train_normal, epochs=40, batch_size=64,
validation_data=(X_test, X_test), shuffle=True)
# Plot loss history
plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
# Predict anomalies/outliers in the test dataset
prediction = autoencoder.predict(X_test)
prediction_loss = keras.losses.mae(prediction, X_test)
loss_threshold = np.percentile(prediction_loss, 98)
# Visualize the prediction loss distribution and threshold
sns.histplot(prediction_loss, bins=30, alpha=0.8)
plt.axvline(x=loss_threshold, color='orange')
# Apply threshold to classify anomalies
threshold_prediction = [0 if i < loss_threshold else 1 for i in prediction_loss]

# Evaluate model performance
print(f'The prediction loss threshold for 2% of outliers is {loss_threshold:.2f}')
print(classification_report(y_test, threshold_prediction))
3)RBM:
import numpy as np
num_user=10
num_movies=5
rating_data=np.random.randint(0,2,size=(num_user,num_movies))
rating_data.shape
def sigmoid(x):
return 1/(1+np.exp(-x))
num_hidden_units=3
weights=np.random.normal(0,0.1,(num_movies,num_hidden_units))
print(weights.shape)
print(weights)
visible_bias=np.zeros(num_movies)
print(visible_bias.shape)
hidden_bias=np.zeros(num_hidden_units)
learning_rate=0.1
print(hidden_bias.shape)
num_epochs = 1000
for epoch in range(num_epochs):
for user_ratings in rating_data:
# Positive phase
hidden_probabilities =sigmoid(np.dot(user_ratings, weights) + hidden_bias)
positive_associations = np.outer(user_ratings, hidden_probabilities)
# Reconstruction
visible_probabilities =sigmoid(np.dot(hidden_probabilities, weights.T) + visible_bias)
# Negative phase
hidden_probabilities_recon=sigmoid (np.dot (visible_probabilities, weights) + hidden_bias )
negative_associations = np.outer(visible_probabilities, hidden_probabilities_recon)
#update weight
weights += learning_rate * (positive_associations - negative_associations)
visible_bias += learning_rate * (user_ratings - visible_probabilities)
hidden_bias += learning_rate * (hidden_probabilities - hidden_probabilities_recon)
user_id = 5
user_ratings =rating_data[user_id]
hidden_probabilities=sigmoid(np.dot(user_ratings, weights) + hidden_bias)
visible_probabilities =sigmoid (np.dot(hidden_probabilities, weights.T) + visible_bias)
predicted_ratings = visible_probabilities
print("Predicted ratings for user", user_id, ":", predicted_ratings)
max_rating = np.max(predicted_ratings)

max_index = np.argmax(predicted_ratings)
print("The maximum predicted rating for user", user_id, "is", max_rating, "at movie", max_index)
4)CNN:
import sys
sys.path.append(r'C:\Users\pradh\OneDrive\Desktop\CNN')
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array,
load_img
import numpy as np
# Define data generators for training and testing
train_datagen = ImageDataGenerator(
rescale=1./255,
shear_range=0.2,
zoom_range=0.2,
horizontal_flip=True
)
test_datagen = ImageDataGenerator(rescale=1./255)
# Path to directories containing training and testing datasets
trainset_path = r"/content/drive/MyDrive/CNN/trainset-20240603T081249Z-001/trainset"
testset_path = r"/content/drive/MyDrive/CNN/testset-20240603T081248Z-001/testset"
# Flow training images in batches of 32 using train_datagen generator
x_train = train_datagen.flow_from_directory(
trainset_path,
target_size=(64, 64),
batch_size=32,
class_mode="categorical"
)
# Flow testing images in batches of 32 using test_datagen generator
x_test = test_datagen.flow_from_directory(
testset_path,
target_size=(64, 64),
batch_size=32,
class_mode="categorical"
)
# Check the class indices of the training set
print(x_train.class_indices)

# Build the CNN model
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(units=128, activation="relu"))
model.add(Dense(units=5, activation="softmax"))
model.summary()
# Compile the model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
# Train the model
model.fit(x_train, steps_per_epoch=55, epochs=1, validation_data=x_test, validation_steps=20)
# Save the model
model.save("animal.h5")
# Load the saved model for testing
model = load_model("animal.h5")
# Prepare an image for testing
img_path = "/content/drive/MyDrive/CNN/testset-20240603T081248Z-001/testset/rats/9k_
(16).jpeg"
target_size = (64, 64)
img = load_img(img_path, target_size=target_size)
x = img_to_array(img)
x = np.expand_dims(x, axis=0)
x /= 255. # Normalize pixel values
y = model.predict(x)
pred = np.argmax(y, axis=1)
# Map predicted index to class label
index = ['bears', 'crows', 'elephants', 'racoons', 'rats']
result = index[pred[0]]
print("Predicted class:", result)
# Check TensorFlow version
import tensorflow as tf
print("TensorFlow version:", tf.__version__)

